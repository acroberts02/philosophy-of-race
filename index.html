<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Algorithms and Bias</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Sora:wght@400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="styles.css">
</head>
<body id="top">
  <header class="site-header">
    <nav class="nav-bar">
      <div class="logo">Algorithms and Bias</div>
      <div class="nav-links">
        <a class="nav-link" href="#introduction">Introduction</a>
        <a class="nav-link" href="#framework">Framework</a>
        <a class="nav-link" href="#cases">Cases</a>
        <a class="nav-link" href="#objections">Objections</a>
        <a class="nav-link" href="#implications">Implications</a>
        <a class="nav-link" href="#citations">Citations</a>
      </div>
    </nav>
  </header>

  <main>
    <section id="hero" class="hero">
      <div class="hero-content">
        <!-- Replace this heading with your real thesis later -->
        <h1>Is AI Racist?</h1>
        <!-- Replace this subheading with your real overview -->
        <p class="hero-subtitle">Can a bunch of 0s and 1s carry the same biases as the most intelligent species on earth?</p>
        <button class="btn primary-btn" data-scroll-target="#introduction">Start Reading</button>
      </div>
    </section>

    <section id="introduction" class="section">
      <div class="section-heading">
        <span class="eyebrow">Overview</span>
        <!-- Replace this heading with your real introduction title -->
        <h2>Introduction</h2>
        <p class="section-subtitle">What is the problem at hand?</p>
      </div>
      <div class="section-body">
        <!-- Replace these paragraphs with your introduction content -->
        <p>In a world increasingly shaped by Artificial Intelligence and related technologies, it is crucial, now more than ever, that we break down and look closely at the way these technologies are handling inherently human biases. From security features to hiring policies, we are putting defining responsibilities in the hands of technologies that have not been researched or safeguarded nearly enough.</p>

        <p>In his article <em>A philosophical analysis of AI and Racism</em>, Lel Jones shares some scary statistics regarding the implications of AI involvement in these technologies. He presents how Google's facial recognition software recognized people of color as gorillas due to the lack of representation in their original dataset and how AI is replacing jobs in fields with disproportionately high levels of employment of people of color. These are obviously severe issues with detrimental consequences but are the algorithms themselves racist? And if so, what can be done?</p>

        <p>Continue exploring this webpage to learn more about how AI works and the implications it is having on real people. I will argue that AI and similar algorithms can be considered racist based on Joshua Glasgow's view of racism as disrespect towards racialized groups. I will explore this argument through case studies, objections, and implications which can be navigated to by scrolling or through clicking each section on the navigation bar.</p>
      </div>
    </section>

    <section id="framework" class="section">
      <div class="section-heading">
        <span class="eyebrow">Context</span>
        <!-- Replace this heading with your real framework title -->
        <h2>Conceptual Framework</h2>
        <p class="section-subtitle">Familiarize yourself with the building blocks of the argument.</p>
      </div>
      <div class="framework-grid">
        <div class="framework-text">
          <h3 class="framework-title">Joshua Glasgow</h3>
          <!-- Replace these paragraphs with your own explanation -->
          <p>In this project I will use Joshua Glasgow’s theory of racism as disrespect to support my argument that AI and related algorithms can be racist. Glasgow understands racism as anything, an action, word, policy, sign, and so on, that disrespects a member of a racialized group on the basis of their membership in that group. In this context, he defines disrespect in moral terms. He says that disrespect is a failure to recognize an individual or group as an autonomous, deserving, morally significant being. On this view, racism is treating that individual or group as less deserving because of their race. Importantly, Glasgow applies this idea to both individual people and larger systems alike.</p>
          <p>This definition is not tied to a particular ‘location’, such as the heart or beliefs, as some of his philosophical counterparts’ views are. That gives it the flexibility necessary to analyze the complex question of AI. According to Glasgow’s definition, there need not be conscious intention or ill will for there to be racism. The focus is widened from explicit hateful actions and slurs to include any case in which someone or something disrespects people on the grounds of race.</p>
        </div>
        <figure class="framework-image">
          <!-- Replace with custom image -->
          <img src="images/glasgow.jpg" alt="Joshua Glasgow portrait">
          <figcaption>Image: Joshua Glasgow, faculty page, Philosophy at Sonoma State University.</figcaption>
        </figure>
      </div>

      <div class="framework-grid">
        <div class="framework-text">
          <h3 class="framework-title">Artificial intelligence and Algorithms</h3>
          <!-- Replace these paragraphs with your second context explanation -->
          <p>It's hard to determine whether AI can be racist without knowing how it works, and let's be honest, figuring out how your sheet of metal is talking to you like a human is not exactly intuitive. So let’s cover some of the basics of AI and machine learning before we try to nail down the argument.</p>
          <p>There are many kinds of AI, but a lot of modern systems, including language models, follow a similar basic idea. You can think of AI as a big mathematical system that is solving for the best possible answer. Each word, phrase, sentence, or whatever chunk the algorithm works with is represented as numbers. When you ask an AI a question, it uses the big mathematical equation to give different possible output scores based on how likely or appropriate they are contextually. In the end it selects the chunk with the highest score to respond with. Think of semantically similar words, for example if I say the word "dog" in my query, the word “pet" is more likely to appear in an appropriate response than the word “pineapple.”</p>
          <p>While using this system to determine its outputs, the algorithm is also shaped by the data it was trained on. This means that when an AI is first built it won’t work at all until it has processed a large batch of training data provided by its developers. Large AI models like ChatGPT, are built on massive training sets scraped from all corners of the internet, from Reddit to Wikipedia. So what AI says and does is, at its core, a reflection of purely human content. This is why we are able to build models for specialty purposes, like diagnosing diseases or telling jokes, by only providing the model with training data pertaining to those subjects.</p>
          <p>With this better understanding, it is easier to see how these models can learn and reproduce human biases, including racist ones.</p>
        </div>
        <figure class="framework-image">
          <!-- Replace with custom image -->
          <img src="images/0x0.jpg.webp" alt="AI Visualization">
          <figcaption>Image: Joseph Tarselli, “From Data to Decisions: How AI and Data Visualization Technologies Are Redefining Business Intelligence,” Forbes (2025).</figcaption>
        </figure>
      </div>
    </section>

    <section id="cases" class="section">
      <div class="section-heading">
        <span class="eyebrow">Examples</span>
        <!-- Replace this heading with your real case study intro -->
        <h2>Case Studies</h2>
        <p class="section-subtitle">Click on each of the case studies to learn more about a real world example of racial biases in algorithms.</p>
      </div>
      <div class="card-grid">
        <article class="card" data-modal-target="case1-modal">
          <!-- Replace this card heading and text with a real example later -->
          <h3>Photo tagging and facial recognition softwares</h3>
          <p>One of the new, shiny features rolled out in the past decade with phone updates was the auto-sorting of photos on your phone. But how well are they truly sorted?</p>
          <!-- Replace with custom image or icon -->
          <img src="images/_83974184_29ba8607-9446-4298-9d9e-d33514811487.jpg.webp" alt="Google algorithm error" class="card-image">
          <button class="btn ghost-btn" type="button">Learn more</button>
        </article>
        <article class="card" data-modal-target="case2-modal">
          <!-- Replace this card heading and text with a real example later -->
          <h3>Risk scores in the criminal justice system</h3>
          <p>Risk scores are supposed to help judges make fair decisions, but what happens when those scores treat some people as more dangerous than others?</p>
          <!-- Replace with custom image or icon -->
          <img src="images/2016-09-23-15540.png" alt="COMPAS example" class="card-image">
          <button class="btn ghost-btn" type="button">Learn more</button>
        </article>
        <article class="card" data-modal-target="case3-modal">
          <!-- Replace this card heading and text with a real example later -->
          <h3>Banking algorithms and loan conditions</h3>
          <p>Banks are turning to algorithms to decide who gets loans, but what happens, when “objective” credit scores quietly repeat our patterns of discrimination?</p>
          <!-- Replace with custom image or icon -->
          <img src="images/AI_in_banking-min.png" alt="Banking AI Visualization" class="card-image">
          <button class="btn ghost-btn" type="button">Learn more</button>
        </article>
      </div>

      <!-- Replace these modal contents with detailed case pages -->
      <div class="modal" id="case1-modal" role="dialog" aria-modal="true" aria-labelledby="case1-title" hidden>
        <div class="modal-content">
          <button class="modal-close" aria-label="Close modal">&times;</button>
          <h3 id="case1-title">Photo tagging and facial recognition softwares</h3>
          <div class="modal-body">
            <p>One of the new, shiny features rolled out in the past decade with phone updates was the auto-sorting of photos on your phone. Most modern phones can search for images containing certain objects or people using a simple keyword, thanks to algorithms that identify these things in each image in your camera roll. But when Google first rolled out this feature, it showed clear racial biases that displayed definite disrespect to users of color. A BBC article titled <em>Google Apologises for Photos App’s Racist Blunder</em> discusses the issue at length.</p>

            <p>A Google user reported photos of him and his dark-skinned friend being sorted into an album and labeled as gorillas. This prompted an immediate response from Google, but of course, raised some much needed questions about the training data provided to the AI. Based on basic knowledge regarding AI, we know that the algorithms are only proficient at recognizing what they have been trained on, so although we may never see the dataset used by Google, we can reasonably assume that it was not diverse enough to produce a publicly releasable model.</p>

            <p>This incorrect labeling of dark-skinned individuals as gorillas was not only disrespectful, but also perpetuated a message of otherness regarding people of color. Intentional or not, the algorithm showed racial bias against people of color and clearly disrespected the individuals in a way that Glasgow would consider racist.</p>
          </div>
          <figure class="modal-figure">
            <!-- Replace with custom image -->
            <img src="images/_83974184_29ba8607-9446-4298-9d9e-d33514811487.jpg.webp" alt="Google algorithm error">
            <figcaption>Image: “Google Apologises for Photos App’s Racist Blunder,” BBC News (2015).</figcaption>
          </figure>
        </div>
      </div>

      <div class="modal" id="case2-modal" role="dialog" aria-modal="true" aria-labelledby="case2-title" hidden>
        <div class="modal-content">
          <button class="modal-close" aria-label="Close modal">&times;</button>
          <h3 id="case2-title">Risk scores in the criminal justice system</h3>
          <div class="modal-body">
            <p>Risk scores are supposed to help judges make fair decisions, but that might not actually be the case when the algorithms generating the scores contain biases. In the article <em>Coded Bias: Decoding Racism in Artificial Intelligence Technologies</em>, Gideon Christian explores a study done by ProPublica to uncover the true implications of these risk assessment algorithms.</p>

            <p>The study focuses on the algorithm, commonly known as COMPAS, and its ability to predict rates of reoffense in defendants. The study found that the algorithm falsely predicted reoffense for black defendants at about twice the rate that it did for white defendants, and further that white defendants were more likely to be classified as “low risk” than their black counterparts. In some cases, black defendants labeled as "high risk” did not reoffend, while white counterparts labeled as “low risk” did reoffend. Essentially, white defendants are receiving quite a bit of slack from the algorithm, while black defendants are facing quite the opposite and this software is still one of the most widely used AI-based risk assessment tools in the U.S. criminal justice system despite these findings.</p>

            <p>Based on the methodology of algorithms, we can attribute these biases to the data on which the model was trained, but that does not negate their consequences. An AI model can never be any better than the data from which it learns, and in this case the data itself is skewed to begin with because of deeply rooted biases in the U.S. justice system. It is easy to chalk up the problem to skewed data, but that does not lessen the very real consequences of the biases. Regardless of the source, the COMPAS algorithm is, by Glasgow’s definition, racist, as it shows disrespect to the black defendants in the criminal justice system and has very real and extreme consequences for the outcome of their lives. A biased algorithm has no place in deciding the future of real individuals.</p>
          </div>
          <figure class="modal-figure">
            <!-- Replace with custom image -->
            <img src="images/2016-09-23-15540.png" alt="COMPAS example">
            <figcaption>Image: Akaash Kambath, “A ‘COMPAS’ That’s Pointing in the Wrong Direction,” Data Science W231 Blog (2021).</figcaption>
          </figure>
        </div>
      </div>

      <div class="modal" id="case3-modal" role="dialog" aria-modal="true" aria-labelledby="case3-title" hidden>
        <div class="modal-content">
          <button class="modal-close" aria-label="Close modal">&times;</button>
          <h3 id="case3-title">Banking algorithms and loan conditions</h3>
          <div class="modal-body">
            <p>Banks are turning to algorithms to decide who gets loans and how those loans will be financed. This is yet another life changing field where people of color are being disadvantaged by a computer. Deciding who is eligible for a loan and under what conditions is a long and complex process that is constantly developing, but when that complexity is boiled down to such a mathematical form we find flaws, and those flaws are certainly not equal ones.</p>

            <p>In his article <em>A Philosophical Analysis of AI and Racism</em>, Lel Jones delves into a section on the implications of algorithms in the banking sector. The main study that he presents is a study out of the University of California, Berkley titled “Consumer-Lending Discrimination in the Era of FinTech.” This study looked at interest rates in the latinx and black populations and found that the banking algorithm was charging latinx and black borrowers with the same FICO credit scores as their white counterparts interest rates that were on average about 7.5 basis points higher on their loans. Jones discusses possible reasoning and counterarguments for this finding, but at its core it is discrimination.</p>

            <p>From Glasgow’s perspective, this finding fits the idea of racial disrespect. The lending algorithm is treating people of color as less trustworthy to pay back their loans by charging them higher interest rates, when their credit scores, another supposedly objective number, show that they have similar patterns as their white counterparts. The algorithms are likely trained on data from banking history in which loans were less accessible to people of color, once again perpetuating past patterns into the future. The inability of these populations to access financial assistance equally is a fault that is absolutely racist and could impact their lives in a cyclical way, further perpetuating biased stereotypes.</p>
          </div>
          <figure class="modal-figure">
            <!-- Replace with custom image -->
            <img src="images/AI_in_banking-min.png" alt="Banking AI Visualization">
            <figcaption>Image: Rosalia Mazza, “AI’s Golden Handshake with Banking: Redefining Trust and Financial Intelligence,” FinTech Weekly (2025).</figcaption>
          </figure>
        </div>
      </div>
    </section>

    <section id="objections" class="section">
      <div class="section-heading">
        <span class="eyebrow">Critiques</span>
        <!-- Replace this heading with your real objections title -->
        <h2>Objections and Replies</h2>
        <p class="section-subtitle">Click on each of the objections to read my response to some of the most common pushbacks against the idea that AI can be racist.</p>
      </div>
      <div class="accordion" aria-label="Objections list">
        <div class="accordion-item">
          <button class="accordion-header" aria-expanded="false">
            <!-- Replace this objection title with your own later -->
            <span>It's just math, not racism.</span>
            <span class="accordion-icon">+</span>
          </button>
          <div class="accordion-panel" hidden>
            <p>Some individuals may argue that because algorithms are just math and numbers at their core, they do not hold the humanity necessary to be racist. The argument is that, despite unfair outcomes, algorithms do not have the ability to portray malicious intent, therefore nullifying their ability to hold racial biases.</p>

            <p>Based on Glasgow's definition of racism, however, there need not be malicious intent or even animacy for there to be racism. In the same way that a nonliving Confederate flag can be racist, an inanimate algorithm can be racist. From access to financial assistance to the ability to get a job, these biased algorithms are disproportionately harmful to people of color. This pattern is a form of disrespect under Glasgow’s definition, meaning that the algorithms themselves can in fact be racist.</p>
          </div>
        </div>
        <div class="accordion-item">
          <button class="accordion-header" aria-expanded="false">
            <!-- Replace this objection title with your own later -->
            <span>The training data is racist, not the algorithm.</span>
            <span class="accordion-icon">+</span>
          </button>
          <div class="accordion-panel" hidden>
            <p>One of the most common topics throughout this webpage is the training data that algorithms are built on. In many of the examples examined, we can tie the biases demonstrated by the algorithms back to the data they were trained on: the banking data before loans were widely accessible to people of color and the criminal justice data from an already skewed system. The argument at hand is that it is the data itself and the old patterns that are racist, not the algorithm.</p>

            <p>While the data is in fact biased in most scenarios, it is the decision to train AI on this data and to perpetuate those patterns that is racist in this scenario. We, as developers and deployers of these algorithms, are electing to use this data and allowing these biased models to be used in high risk scenarios. According to Glasgow, the algorithms can now be considered racist because they are out in the world perpetuating disrespect in their own fashion. If a parent taught their racist beliefs to a child and the child perpetuated them, we would not say that the child is not racist because they learned it from their parent, we would say that now both the parent and the child are racist. In that same fashion, the algorithms trained on biased data are displaying disrespect and are therefore racist in their own independent manner. The biased pattern could have ceased with the data, but is instead becoming perpetual through these technologies.</p>
          </div>
        </div>
        <div class="accordion-item">
          <button class="accordion-header" aria-expanded="false">
            <!-- Replace this objection title with your own later -->
            <span>Calling an inanimate object racist makes the word less meaningful.</span>
            <span class="accordion-icon">+</span>
          </button>
          <div class="accordion-panel" hidden>
            <p>A common worry with the expansion of the term “racist” to cover more abstract concepts is that the word will become overused and the definition watered down. Many fear drawing attention away from more blatant forms of racism, like slurs and hate groups. The concern in the long run is that through this potential overuse, people may begin to take accusations of racism less seriously.</p>

            <p>While this is a very real concern, the way in which we evaluate whether an offense is “serious” enough to be considered racist is the deeper issue. The most common examples of extreme racism that you hear are ones rooted in hate, like those mentioned above. Without negating those forms of racism at all, it is essential that we also take into account the impacts of a scenario when deeming it “worthy enough” to be called racist. The impacts that biased algorithms have on people’s lives are very real and very widespread, and the sectors in which these algorithms are being used, like banking, criminal justice, and employment, are certainly not ones to be taken lightly. Based on Glasgow's theory of disrespect, the impacts that biased algorithms have on people of color are disadvantageous and disrespectful, making it appropriate to deem them racist.</p>
          </div>
        </div>
      </div>
    </section>

    <section id="implications" class="section">
      <div class="section-heading">
        <span class="eyebrow">Takeaways</span>
        <!-- Replace this heading with your real implications title -->
        <h2>Implications</h2>
      </div>
      <div class="section-body">
        <!-- Replace this text with your concluding thoughts -->
        <p>Given Glasgow’s definition of racism as discussed on this webpage, biased algorithms are not just buggy technologies; they are racist. This is racism that is affecting the lives of countless individuals in very real ways day in and day out. People of color have to face these biases when trying to get a loan, defend themselves in court, or even just use their phone for leisure. And those are just the cases discussed on this webpage, not to mention the many other places where AI is being integrated. We cannot go on any longer blaming this racism on developing technology and messy code.</p>

        <p>Correcting these issues needs to start with a redistribution of responsibility. Someone out there chose to train the model on that data, someone out there chose to deploy that model, and someone out there chose to keep that model in use despite the findings at hand. The companies and people responsible for these decisions are no less racist than the algorithm itself. AI does not have the self-awareness to shut itself down or correct its ways, at least not yet, so in the meantime, we as developers must take on that responsibility. If action is not taken, we will be handing the ghosts of a racist past a one-way ticket to the future and potentially infinite perpetuity.</p>

        <p>The goal, now that we are beginning to recognize the biases in AI, is to retrain these systems to ensure that they do not continue treating members of racialized groups as less visible, trustworthy, or deserving. This will require us to ask critical questions about how we build systems, how we train them, and most importantly how we use them. Looking at racism as disrespect through Glasgow’s view allows us to see this vision clearly. These tools can be truly revolutionary, but until they no longer disrespect people on a racial basis, we will remain stuck in the past.</p>
      </div>
    </section>

    <section id="citations" class="section">
      <div class="section-heading">
        <span class="eyebrow">Sources</span>
        <!-- Replace this heading with your real citations title -->
        <h2>Citations</h2>
        <p class="section-subtitle">Below are the sources used for both ideas and images in the contents of the website.</p>
      </div>
      <ul class="citation-list">
        <li>Christian, C. <em>Coded bias: Decoding racism in artificial intelligence technologies.</em> <em>Coding, Computational Modeling, and Equity in Mathematics Education Symposium</em>, St. Catharines, ON, Canada, 2023.</li>
        <li>Glasgow, Joshua. <em>Racism as Disrespect.</em> <em>Ethics</em>, vol. 120, no. 1, 2009, pp. 64–93. JSTOR, https://doi.org/10.1086/648588. Accessed 8 Dec. 2025.</li>
        <li><em>Google Apologises for Photos App’s Racist Blunder.</em> <em>BBC News</em>, 1 July 2015, www.bbc.com/news/technology-33347866. Accessed 9 Dec. 2025.</li>
        <li><em>Joshua Glasgow.</em> <em>Philosophy at Sonoma State University</em>, Sonoma State University, philosophy.sonoma.edu/faculty-staff/joshua-glasgow. Accessed 8 Dec. 2025.</li>
        <li>Jones, Lel. <em>A philosophical analysis of AI and racism.</em> <em>Stance: An International Undergraduate Philosophy Journal</em>, vol. 13, 2020, pp. 36-46.</li>
        <li>Kambath, Akaash. <em>A ‘COMPAS’ That’s Pointing in the Wrong Direction.</em> <em>Data Science W231: Behind the Data: Humans and Values</em>, School of Information, University of California, Berkeley, 9 July 2021, blogs.ischool.berkeley.edu/w231/2021/07/09/a-compas-thats-pointing-in-the-wrong-direction/.</li>
        <li>Mazza, Rosalia. <em>AI’s Golden Handshake with Banking: Redefining Trust and Financial Intelligence.</em> <em>FinTech Weekly</em>, 18 Jan. 2025, www.fintechweekly.com/magazine/articles/ai-in-banking.</li>
        <li>Tarselli, Joseph. <em>From Data to Decisions: How AI and Data Visualization Technologies Are Redefining Business Intelligence.</em> <em>Forbes</em>, Forbes Media, 10 Mar. 2025, www.forbes.com/councils/forbestechcouncil/2025/03/10/from-data-to-decisions-how-ai-and-data-visualization-technologies-are-redefining-business-intelligence/.</li>
      </ul>
    </section>

    <div class="back-top-wrap">
      <a class="btn back-to-top" href="#top">Back to top</a>
    </div>
  </main>

  <footer class="footer">
    <p>Created by Alice Roberts for Philosophy 220 Final Project</p>
  </footer>

  <script src="script.js"></script>
</body>
</html>
